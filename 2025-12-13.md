# Daily Progress Notes

## AI
**혼공머신 07-03 : Neural Networks**

### The Core Problem
Why doesn't accuracy improve proportionally with loss decrease?

**Loss (Continuous)**
- Measures probability distribution quality + confidence
- 0.51 vs 0.99 → Huge difference in loss value

**Accuracy (Discrete)** 
- Only measures "correct or not"
- 0.51 vs 0.99 → Same accuracy contribution (both correct)

### The Pattern
Training early: Loss↓↓↓, Accuracy↑↑↑ (proportional)
Training late: Loss↓, Accuracy→ (not proportional)

Why? Easy samples already correct → Accuracy saturates
But model still improving confidence → Loss keeps decreasing

### Why This Matters
- **Overfitting detection**: Watch validation loss, not just accuracy
- **Model calibration**: "70% confident" should mean actually 70% correct
- Loss tells you model's real learning progress

### Applied Concept
Probability concentration = Confidence = Low entropy

## Japanese
**JLPT N3(Hackers)**
- vocabulary (textbook)
- JLPT app 