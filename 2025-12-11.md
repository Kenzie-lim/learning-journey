# Daily Progress Notes

## AI
**혼공머신 07 : Neural Networks**

### The Core Problem
1960s Perceptron couldn't learn multi-layer networks. Why?
→ Step functions have no gradient (0 or undefined)
→ Can't calculate how to adjust weights

### The Solution
Continuous activation functions (sigmoid, ReLU) provide smooth gradients
→ Backpropagation possible
→ Deep learning revolution

### Applied in
Fine-tuning Stable Diffusion by freezing early layers
= Keeping general feature detection, only training style-specific params

**Statquest - Neural Networks & AI : 01**
- Fundamental Concepts
- tutorial PyTorch


## Japanese
**JLPT N3(Hackers)**
- vocabulary (textbook)