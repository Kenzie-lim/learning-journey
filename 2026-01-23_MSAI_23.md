## Overview

### Context
Two-part session: (1) Stanford AI Index Report 2025 benchmark landscape review, (2) Continued Azure ML + Stable Diffusion practice with advanced model/LoRA management. Builds on ML fundamentals and previous SD setup (Jan 20-21).

### Objective
Understand AI benchmark ecosystem and evaluation methods; master SD model management, WebUI parameters, and automated workflows.

---

## Part 1: Stanford AI Index Report 2025

### Big Picture: 8 Key Takeaways
1-4: Technical progress (performance↑, real-world penetration, investment↑)
5-8: Ecosystem (Responsible AI, optimism gap, efficiency, regulation)

### Benchmark Landscape Map

| Category | Benchmarks | What It Measures | Current State |
|----------|------------|------------------|---------------|
| **Language Understanding** | MMLU, MMLU-Pro | Knowledge across 57 subjects | MMLU saturated (92.3%) |
| **Language Generation** | LM Arena, Arena-Hard, WildBench, MixEval | Open-ended response quality | MixEval balances auto+human |
| **Math** | MATH, GSM8K, IMO, FrontierMath | Mathematical reasoning | US-China gap: 24.3%→1.6% |
| **Advanced Reasoning** | GPQA, ARC-AGI, HLE | PhD-level + frontier problems | o3: ARC-AGI 75.7%, HLE 8.8% |
| **Coding** | HumanEval, SWE-Bench, BigCodeBench | Code generation + real repo fixes | SWE-Bench: 4.4%→71.7% |
| **RAG** | MTEB, RULER, HELMET | Retrieval + long context | RULER: most models fail at 32K+ |
| **Responsible AI** | FACTS, HHEM, SimpleQA | Hallucination, factuality | New in 2024 |
| **Multimodal** | MMMU, MathVista, VCR, MVBench | Image/video understanding | MMMU +18.8%p |
| **AI Agents** | WebArena, OSWorld | Real-world task execution | Still low performance |
| **Robotics** | DROID, Open X-Embodiment | Robot learning generalization | New datasets 2024 |

### Evaluation Method Classification

| Method | Benchmarks | Characteristics |
|--------|------------|-----------------|
| **Fully Automated** | MMLU, MATH, HumanEval, GPQA, ARC-AGI | Has ground truth → auto-scoring, reproducible |
| **LLM-as-Judge** | Arena-Hard-Auto, WildBench, FACTS, HHEM | Model judges quality, efficient but biased |
| **Human Evaluation** | LM Arena (Chatbot Arena) | Real users A/B select → Elo ranking, costly |
| **Hybrid** | MixEval, SWE-Bench | Combines auto + judgment for balance |

### Benchmark Types: Dataset vs Evaluation Program

| Type | Examples | Characteristics |
|------|----------|-----------------|
| **Dataset** | Celeb-DF, MPII, ImageNet, COCO | Anyone can download and test; self-reported results |
| **Evaluation Program** | NIST FRVT | Submit algorithm → third party tests; more objective |

### Key Metrics Explained

| Metric | Formula | When to Use |
|--------|---------|-------------|
| **AUC** | Area under ROC curve | Imbalanced data, threshold-independent evaluation |
| **Precision** | TP / (TP + FP) | When FP is costly (spam filter) |
| **Recall** | TP / (TP + FN) | When FN is costly (cancer detection) |
| **Top-1 Accuracy** | Highest prob = correct? | Classification tasks |
| **Top-5 Accuracy** | Correct in top 5? | Multi-class with ambiguity |

Note: Accuracy misleading for imbalanced data (99% "normal" → predict all normal = 99% accuracy but useless)

### Terminology Clarification

| Term | Meaning | Context |
|------|---------|---------|
| **Inference** | Running trained model on new data | "Inference time", model.predict() |
| **Reasoning** | Logical step-by-step thinking | Chain-of-thought, o1/o3 models |
| **Test-Time Compute (TTC)** | Extra computation during inference | o1, o3 use this for complex reasoning |

### Benchmark Saturation Problem
- When benchmarks get "solved" (90%+), they lose discriminative power
- Solutions: Harder versions (MMLU→MMLU-Pro), new benchmarks (FrontierMath, HLE)
- Risk: Models may "hack" benchmarks (memorize patterns, not truly generalize)

---

## Part 2: Stable Diffusion Advanced Practice

### Flow

1. Model Installation (SDXL)
2. WebUI Parameter Deep Dive
3. LoRA Installation & Usage
4. Wrong LoRA Concept

ETC
- Sampling Steps (Diffusion perspective)
- VRAM Management
- Sampler Selection
- UI Mode Matching

